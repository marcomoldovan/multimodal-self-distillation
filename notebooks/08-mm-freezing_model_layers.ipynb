{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers import BertModel\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertEmbeddings, BertEncoder, BertPooler\n",
    "from transformers.models.hubert.modeling_hubert import HubertPreTrainedModel, HubertFeatureEncoder, HubertFeatureProjection, HubertEncoder, HubertModel\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('google/bert_uncased_L-4_H-128_A-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_module(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param.shape)\n",
    "        print(param.requires_grad)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters = lambda model : {'requires_grad':sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6,\n",
    "                                   'does_not_require_grad':sum(p.numel() for p in model.parameters() if not p.requires_grad)/1e6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_layers = 1\n",
    "\n",
    "for child in model.children():\n",
    "    print(child._get_name())\n",
    "    if isinstance(child, BertEmbeddings):\n",
    "        freeze_whole_model(child)\n",
    "    elif isinstance(child, )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model.named_children():\n",
    "    print(child.embeddings)\n",
    "    if isinstance(child, BertEmbeddings):\n",
    "        print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO to test this just inserst all the code from encoder_components.py in an above cell\n",
    "# anything with src... cannot be imported for some reason\n",
    "\n",
    "bi_enc_no_conv = BiEncoderSpeechTextModelWithoutFeatureEncoder()\n",
    "bi_enc = BiEncoderSpeechTextModel()\n",
    "mm_enc = MultiModalSpeechTextEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_enc_no_conv.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_enc.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers_except_last(model, n_layers_to_train=1):\n",
    "    for name, child in model.named_children():\n",
    "        if name == 'transformer':\n",
    "            continue\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.hubert.modeling_hubert import HubertPreTrainedModel, HubertFeatureEncoder, HubertFeatureProjection, HubertEncoder, HubertPositionalConvEmbedding\n",
    "from torch.nn import LayerNorm, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, trainable_layers=0):\n",
    "    \"\"\"Trainable layers refers to the number of trainable attention layers\n",
    "        in the network. If trainable layers > 0, then the corresponding projection\n",
    "        head will also be trainable. In case of a Bi-Encoder only components of\n",
    "        speech model will be trainable, the text model will always be frozen.\n",
    "\n",
    "    Args:\n",
    "        model (\n",
    "            BiEncoderSpeechTextModelWithoutFeatureEncoder,\n",
    "            BiEncoderSpeechTextModel,\n",
    "            MultiModalSpeechTextEncoder\n",
    "            ): The model to be frozen.\n",
    "        trainablelayers (int, optional): How many attention layers in the speech or\n",
    "            multimodal encoder to train. Defaults to 0.\n",
    "    \"\"\"\n",
    "    print(f\"Parameters before freezing: {count_parameters(model)}\")\n",
    "    \n",
    "    for _, child in model.named_children():\n",
    "        \n",
    "        # standard BERT as text model\n",
    "        if isinstance(child, BertModel):\n",
    "            freeze_module(child)\n",
    "        \n",
    "        # modules for the multimodal encoder\n",
    "        elif isinstance(child, BertEmbeddingsWrapper):\n",
    "            freeze_module(child)\n",
    "        elif isinstance(child, HubertConvFeatureExtractorWrapper):\n",
    "            freeze_module(child)\n",
    "        elif isinstance(child, HubertFeatureProjectionWrapper):\n",
    "            freeze_module(child)\n",
    "        elif isinstance(child, BertEncoderWrapper):          \n",
    "            for na, ch in child.named_children():\n",
    "                for n, c in ch.named_children():\n",
    "                    if isinstance(c, torch.nn.ModuleList):\n",
    "                        for i, _ in enumerate(c._modules):\n",
    "                                if i < (len(c._modules) - trainable_layers):\n",
    "                                    freeze_module(c[i])\n",
    "        elif isinstance(child, HubertPooler) or isinstance(child, BertPoolerWrapper):\n",
    "            pass\n",
    "        \n",
    "        # modules for the speech encoder without convolution\n",
    "        elif isinstance(child, HubertModelWithoutFeatureEncoder): # done\n",
    "            for na, ch in child.named_children():\n",
    "                if isinstance(ch, HubertFeatureProjectionWrapper):\n",
    "                    freeze_module(ch)\n",
    "                elif isinstance(ch, HubertEncoderWrapper):\n",
    "                    for n, c in ch.named_children():\n",
    "                        for n_enc, c_enc in c.named_children():\n",
    "                            if isinstance(c_enc, LayerNorm):\n",
    "                                freeze_module(c_enc)\n",
    "                            elif isinstance(c_enc, Dropout):\n",
    "                                freeze_module(c_enc)\n",
    "                            elif isinstance(c_enc, torch.nn.ModuleList):\n",
    "                                for i, _ in enumerate(c_enc._modules):\n",
    "                                    if i < (len(c_enc._modules) - trainable_layers):\n",
    "                                        freeze_module(c_enc[i])\n",
    "                elif isinstance(ch, HubertPooler):\n",
    "                    pass\n",
    "        \n",
    "        # modules for the HuBERT speech encoder with convolution and pooler             \n",
    "        elif isinstance(child, HubertModelWithPooler): # done\n",
    "            for na, ch in child.named_children():\n",
    "                if isinstance(ch, HubertModel):\n",
    "                    freeze_module(ch.feature_extractor)\n",
    "                    freeze_module(ch.feature_projection)\n",
    "                    for n, c in ch.encoder.named_children():\n",
    "                        if isinstance(c, HubertPositionalConvEmbedding):\n",
    "                            freeze_module(c)\n",
    "                        elif isinstance(c, LayerNorm):\n",
    "                            freeze_module(c)\n",
    "                        elif isinstance(c, Dropout):\n",
    "                            freeze_module(c)\n",
    "                        elif isinstance(c, torch.nn.ModuleList):\n",
    "                            for i, _ in enumerate(c._modules):\n",
    "                                if i < (len(c._modules) - trainable_layers):\n",
    "                                    freeze_module(c[i])\n",
    "                if isinstance(ch, HubertPooler):\n",
    "                    pass\n",
    "                \n",
    "    print(f\"Parameters after freezing: {count_parameters(model)}\")\n",
    "    \n",
    "freeze_model(mm_enc, trainable_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2beefd3eff767b677397e49c617fe9eb55de17b3640af7353d98162718fbf92"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('speech-segment-retrieval': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
