{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data2vec\n",
    "\n",
    "Reconstructing the criterion as per https://github.com/facebookresearch/fairseq/blob/main/examples/data2vec/models/data2vec_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([20, 384, 128])\n",
      "y shape:  torch.Size([20, 384, 128])\n",
      "loss:  tensor([[55.5846, 55.4130, 53.9140,  ..., 52.7618, 55.7299, 54.5257],\n",
      "        [54.7572, 54.4994, 55.6759,  ..., 55.4732, 54.7223, 53.5696],\n",
      "        [54.4100, 52.8356, 53.7533,  ..., 55.7398, 52.6998, 55.2166],\n",
      "        ...,\n",
      "        [53.4885, 53.2036, 56.9283,  ..., 53.5202, 55.3001, 53.7072],\n",
      "        [54.7876, 55.1792, 54.9925,  ..., 55.5256, 53.9834, 56.4594],\n",
      "        [55.0112, 55.7622, 54.9119,  ..., 54.9068, 52.6478, 54.5516]],\n",
      "       grad_fn=<SumBackward1>) loss shape:  torch.Size([20, 384])\n",
      "{'losses': {'main': tensor(419955.5938, grad_fn=<MulBackward0>)}, 'sample_size': 7680}\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "seq_len = 384\n",
    "batch_size = 20\n",
    "attention_layers = 12\n",
    "\n",
    "has_faiss_format = False\n",
    "batch_norm_target_layer = True\n",
    "instance_norm_target_layer = True\n",
    "layer_norm_target_layer = True\n",
    "layer_norm_targets = True\n",
    "instance_norm_targets = True\n",
    "\n",
    "projector = nn.Sequential(\n",
    "    nn.Linear(embedding_dim, embedding_dim * 2),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(embedding_dim * 2, embedding_dim * 4),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(embedding_dim * 4, embedding_dim)\n",
    ")\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embedding_dim) # (batch_size, sequence_length, hidden_size)\n",
    "x = projector(x) # (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "print('x shape: ', x.shape)\n",
    "\n",
    "# take k last layers\n",
    "k = 4\n",
    "y = [torch.randn(batch_size, seq_len, embedding_dim)] * attention_layers # (batch_size, sequence_length, hidden_size) * attention_layers\n",
    "y = y[-k:]\n",
    "\n",
    "# B: batch size, T: sequence length, C: hidden size\n",
    "\n",
    "if not has_faiss_format:\n",
    "    y = [tl.permute(1, 0, 2) for tl in y] # BTC -> TBC\n",
    "\n",
    "permuted = False\n",
    "if  batch_norm_target_layer or instance_norm_target_layer:\n",
    "    y = [tl.permute(1, 2, 0) for tl in y]  # TBC -> BCT\n",
    "    permuted = True\n",
    "\n",
    "if batch_norm_target_layer:\n",
    "    y = [\n",
    "        F.batch_norm(\n",
    "            tl.float(), running_mean=None, running_var=None, training=True\n",
    "        )\n",
    "        for tl in y\n",
    "    ]\n",
    "\n",
    "if instance_norm_target_layer:\n",
    "    y = [F.instance_norm(tl.float()) for tl in y]\n",
    "\n",
    "if permuted:\n",
    "    y = [tl.transpose(1, 2) for tl in y]  # BCT -> BTC\n",
    "\n",
    "if layer_norm_target_layer:\n",
    "    y = [F.layer_norm(tl.float(), tl.shape[-1:]) for tl in y]\n",
    "\n",
    "y = sum(y) / len(y)\n",
    "\n",
    "if not permuted:\n",
    "    y = y.transpose(0, 1)\n",
    "\n",
    "if layer_norm_targets:\n",
    "    y = F.layer_norm(y.float(), y.shape[-1:])\n",
    "\n",
    "if instance_norm_targets:\n",
    "    y = F.instance_norm(y.transpose(1, 2)).transpose(1, 2)\n",
    "    \n",
    "print('y shape: ', y.shape)\n",
    "\n",
    "loss_beta = 1.0\n",
    "loss_scale = 1.0\n",
    "sz = x.size(-1)\n",
    "\n",
    "loss = F.smooth_l1_loss(\n",
    "                x.float(), y.float(), reduction=\"none\", beta=loss_beta\n",
    "            ).sum(dim=-1)\n",
    "print('loss: ', loss, 'loss shape: ', loss.shape)\n",
    "\n",
    "result = {\n",
    "            \"losses\": {\n",
    "                \"main\": loss.sum() / math.sqrt(sz)\n",
    "                if loss_scale <= 0\n",
    "                else loss.sum() * loss_scale,\n",
    "            },\n",
    "            \"sample_size\": loss.numel(),\n",
    "        }\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ac8a2bc69d4b2bdc42aaccd63f192d886c476dacd93adfa548f17911c905576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
