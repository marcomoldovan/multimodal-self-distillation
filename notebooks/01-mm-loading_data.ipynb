{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import PerceiverFeatureExtractor, PerceiverTokenizer, PerceiverForMultimodalAutoencoding, PerceiverForImageClassificationFourier\n",
    "from datasets import load_dataset\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multimodal inputs\n",
    "images = torch.randn((1, 16, 3, 224, 224))\n",
    "audio = torch.randn((1, 30720, 1))\n",
    "inputs = dict(image=images, audio=audio, label=torch.zeros((images.shape[0], 700)))\n",
    "\n",
    "multimodal_perceiver = PerceiverForMultimodalAutoencoding.from_pretrained(\"deepmind/multimodal-perceiver\")\n",
    "\n",
    "# in the Perceiver IO paper, videos are auto-encoded in chunks\n",
    "# each chunk subsamples different index dimensions of the image and audio modality decoder queries\n",
    "nchunks = 128\n",
    "image_chunk_size = np.prod((16, 224, 224)) // nchunks\n",
    "audio_chunk_size = audio.shape[1] // multimodal_perceiver.config.samples_per_patch // nchunks\n",
    "# process the first chunk\n",
    "chunk_idx = 0\n",
    "subsampling = {\n",
    "    \"image\": torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)),\n",
    "    \"audio\": torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)),\n",
    "    \"label\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = multimodal_perceiver(inputs=inputs, subsampled_output_points=subsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_perceiver = PerceiverForImageClassificationFourier.from_pretrained('deepmind/vision-perceiver-fourier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_preprocessor = multimodal_perceiver.perceiver.input_preprocessor\n",
    "multimodal_encoder = multimodal_perceiver.perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_preprocessor_outputs = multimodal_preprocessor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52097, 704])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_preprocessor_outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_preprocessor_vision = multimodal_preprocessor.modalities[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((32, 3, 224, 224))\n",
    "\n",
    "vision_preprocessor = vision_perceiver.perceiver.input_preprocessor\n",
    "\n",
    "image_out, _, _ = vision_preprocessor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50176, 261])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "perceiver_tokenizer = PerceiverTokenizer.from_pretrained('deepmind/language-perceiver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.utils.logging.get_verbosity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base')\n",
    "\n",
    "input_values = np.random.randn(16000)\n",
    "\n",
    "pad_to_multiple_of = 96\n",
    "\n",
    "max_length = input_values.shape[0]\n",
    "\n",
    "max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "\n",
    "padded = extractor(input_values, pad_to_multiple_of=96, padding='longest', return_tensors='pt', sampling_rate=16000)\n",
    "\n",
    "print(padded.input_values.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = perceiver_tokenizer(\"This is an incomplete sentence where some words are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = load_dataset(\"wikipedia\", \"20220301.en\", cache_dir=\"E:/Datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6458670"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikipedia['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the Perceiver IO paper, videos are auto-encoded in chunks\n",
    "# each chunk subsamples different index dimensions of the image and audio modality decoder queries\n",
    "nchunks = 128\n",
    "image_chunk_size = np.prod((1, 224, 224)) // nchunks # np.prod((16, 224, 224)) // nchunks\n",
    "audio_chunk_size = audio.shape[1] // model.config.samples_per_patch // nchunks\n",
    "# process the first chunk\n",
    "chunk_idx = 0\n",
    "subsampling = {\n",
    "    \"image\": torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)),\n",
    "    \"audio\": torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)),\n",
    "    \"label\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, _, _ = model(inputs, subsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 434)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(\"(1920, 434)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractPreprocessor(torch.nn.Module):\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        \"\"\"Returns size of preprocessor output.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class PerceiverTextPreprocessor(AbstractPreprocessor):\n",
    "    \"\"\"\n",
    "    Text preprocessing for Perceiver Encoder. Can be used to embed `inputs` and add positional encodings.\n",
    "    The dimensionality of the embeddings is determined by the `d_model` attribute of the configuration.\n",
    "    Args:\n",
    "        config ([`PerceiverConfig`]):\n",
    "            Model configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int,\n",
    "        vocab_size: int,\n",
    "        max_position_embeddings: int\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.position_embeddings = torch.nn.Embedding(max_position_embeddings, d_model)\n",
    "\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.d_model\n",
    "\n",
    "    def forward(self, inputs: torch.LongTensor) -> torch.FloatTensor:\n",
    "        embeddings = self.embeddings(inputs)\n",
    "\n",
    "        seq_length = inputs.shape[1]\n",
    "        position_ids = torch.arange(0, seq_length, device=inputs.device)\n",
    "        embeddings = embeddings + self.position_embeddings(position_ids)\n",
    "\n",
    "        return embeddings, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PerceiverTokenizer.from_pretrained('deepmind/language-perceiver')\n",
    "preprocessor = PerceiverTextPreprocessor(d_model=512, vocab_size=262, max_position_embeddings=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer(text, return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 46, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, _, _ = preprocessor(tokens)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr_dummy (C:\\Users\\marco\\.cache\\huggingface\\datasets\\patrickvonplaten___librispeech_asr_dummy\\clean\\2.1.0\\f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.44it/s]\n"
     ]
    }
   ],
   "source": [
    "libri_dummy = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(libri_dummy['validation'][0]['audio']['array'])\n",
    "# libri_dummy['validation'][0]['audio']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.randn(3, 64, 64)\n",
    "y = torch.randn(3, 64, 64)\n",
    "z = torch.stack([x, y], dim=0)\n",
    "\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('multimodal-ssl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ac8a2bc69d4b2bdc42aaccd63f192d886c476dacd93adfa548f17911c905576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
