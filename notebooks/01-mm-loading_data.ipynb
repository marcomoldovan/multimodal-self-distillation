{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import PerceiverFeatureExtractor, PerceiverTokenizer, PerceiverForMultimodalAutoencoding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multimodal inputs\n",
    "images = torch.randn((1, 16, 3, 224, 224))\n",
    "audio = torch.randn((1, 30720, 1))\n",
    "inputs = dict(image=images, audio=audio, label=torch.zeros((images.shape[0], 700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 35.9kB [00:00, 8.97MB/s]                   \n",
      "Downloading metadata: 30.4kB [00:00, 3.04MB/s]                   \n",
      "Reusing dataset wikipedia (E:/Datasets/wikipedia\\20220301.en\\2.0.0\\aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "100%|██████████| 1/1 [03:09<00:00, 189.21s/it]\n"
     ]
    }
   ],
   "source": [
    "wikipedia = load_dataset(\"wikipedia\", \"20220301.en\", cache_dir=\"E:/Datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6458670"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikipedia['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerceiverForMultimodalAutoencoding.from_pretrained(\"deepmind/multimodal-perceiver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the Perceiver IO paper, videos are auto-encoded in chunks\n",
    "# each chunk subsamples different index dimensions of the image and audio modality decoder queries\n",
    "nchunks = 128\n",
    "image_chunk_size = np.prod((16, 224, 224)) // nchunks\n",
    "audio_chunk_size = audio.shape[1] // model.config.samples_per_patch // nchunks\n",
    "# process the first chunk\n",
    "chunk_idx = 0\n",
    "subsampling = {\n",
    "    \"image\": torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)),\n",
    "    \"audio\": torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)),\n",
    "    \"label\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs=inputs, subsampled_output_points=subsampling, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.d_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 512])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.instance_norm(y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_head = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model.config.d_latents, model.config.d_latents * 2), \n",
    "    torch.nn.GELU(), \n",
    "    torch.nn.Linear(model.config.d_latents * 2, model.config.d_latents)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784, 512])\n",
      "torch.Size([1, 784, 512])\n",
      "torch.Size([1, 784, 512])\n"
     ]
    }
   ],
   "source": [
    "# https://www.baeldung.com/cs/instance-vs-batch-normalization\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = y[-5:]  # take the last k transformer layers\n",
    "    # Follow the same layer normalization procedure for text and vision\n",
    "    y = [torch.layer_norm(tl.float(), tl.shape[-1:]) for tl in y]\n",
    "    y = sum(y) / len(y)\n",
    "    if True: # noralize targets\n",
    "        y = torch.layer_norm(y.float(), y.shape[-1:])\n",
    "\n",
    "    # # # Use instance normalization for audio\n",
    "    # y = [torch.nn.functional.instance_norm(tl.float()) for tl in y]\n",
    "    # y = sum(y) / len(y)\n",
    "    # if True: # normalize targets\n",
    "    #     y = torch.nn.functional.instance_norm(y.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "print(outputs.hidden_states[-1:][0].size())\n",
    "x = regression_head(outputs.hidden_states[-1:][0])\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = torch.nn.SmoothL1Loss()\n",
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0948, grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0948, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1(x.float(), y.float()).sum(dim=-1).sum().div(x.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.8993, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = model.perceiver.input_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50176, 243])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.modalities[\"image\"](images)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1920, 401])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.modalities[\"audio\"](audio)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_prcessed, modality_sizes, inputs_without_pos = preprocessor({\"image\": images, \"audio\": audio, \"label\": torch.zeros((images.shape[0], 700))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('perceiver-data2vec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "356f0f1e9d918dde982ecf27c70cbfdd6ae28858b3c646ccfe6075c66f643012"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
