{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Union\n",
    "from transformers import BertTokenizerFast, Wav2Vec2FeatureExtractor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriPreprocessor:\n",
    "  def __init__(self):\n",
    "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    self.extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base')\n",
    "    self.feature_encoder = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base').feature_extractor\n",
    "    self.feature_encoder.to(self.device)\n",
    "    self.feature_encoder.eval()\n",
    "  \n",
    "  \n",
    "  def speech_file_to_array_fn(self, data):\n",
    "    #import soundfile as sf\n",
    "    #speech_array, sampling_rate = sf.read(data[\"file\"])\n",
    "    data['speech'] = data['audio']['array']\n",
    "    data['sampling_rate'] = data['audio']['sampling_rate']\n",
    "    #data[\"target_text\"] = data[\"text\"]\n",
    "    return data\n",
    "    \n",
    "    \n",
    "  def prepare_dataset(self, data): \n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(data['sampling_rate'])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {self.extractor.sampling_rate}.\"\n",
    "\n",
    "    # extract and pad input values\n",
    "    input_values = self.extractor(data['speech'], sampling_rate=data['sampling_rate'][0])\n",
    "    data['input_values'] = input_values.input_values\n",
    "    padded_input_values = self.extractor.pad(input_values, return_tensors='pt')\n",
    "    \n",
    "    # compute the latent features from the conv module\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "      input_values = padded_input_values['input_values'].to(self.device)\n",
    "      latent_features = self.feature_encoder(input_values).transpose(1, 2)\n",
    "      latent_features = latent_features.cpu().numpy()\n",
    "      data['latent_features'] = latent_features\n",
    "    \n",
    "    # tokenize text\n",
    "    tokenized_batch = self.tokenizer(data['text'], padding='longest', max_length=128, pad_to_max_length=False)\n",
    "    data['input_ids'] = tokenized_batch['input_ids']\n",
    "    data['attention_mask_text'] = tokenized_batch['attention_mask']\n",
    "    data['token_type_ids_text'] = tokenized_batch['token_type_ids']\n",
    "    \n",
    "    return data\n",
    "  \n",
    "  \n",
    "  def pad_latent_features(self, latent_features, padding='longest', return_tensors=\"pt\"):\n",
    "    padding_value = 0.0\n",
    "    if padding == 'longest':\n",
    "      longest_latent_feature = max(len(item['latent_features']) for item in latent_features)\n",
    "\n",
    "    padded_features = []\n",
    "    for item in latent_features:\n",
    "      latent_features_as_ndarray = np.array(item['latent_features']).astype(np.float32)\n",
    "      padded_item = np.pad(latent_features_as_ndarray, \n",
    "                           ((0, longest_latent_feature - latent_features_as_ndarray.shape[0]), (0, 0)), \n",
    "                           mode='constant', \n",
    "                           constant_values=padding_value)\n",
    "      if return_tensors == \"pt\":\n",
    "        padded_item = torch.from_numpy(padded_item).to(torch.float32)\n",
    "      padded_features.append(padded_item)\n",
    "      \n",
    "    if return_tensors == \"pt\":\n",
    "      padded_features = torch.stack(padded_features)\n",
    "      \n",
    "    return padded_features\n",
    "\n",
    "\n",
    "  def __call__(\n",
    "    self,\n",
    "    batch: List[Dict[str, Union[List[int], torch.Tensor]]],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function to be used when training with PyTorch Lightning.\n",
    "    Returns:\n",
    "        :obj:`Dict[str, torch.Tensor]`: A dictionary of tensors containing the collated features.\n",
    "    \"\"\" \n",
    "    latent_features = [{'latent_features': feature['latent_features']} for feature in batch]\n",
    "    # input_values = [{'input_values': feature['input_values']} for feature in batch]\n",
    "    input_sentences = [{'input_ids': feature['input_ids']} for feature in batch]\n",
    "    \n",
    "    text_batch = self.tokenizer.pad(\n",
    "        input_sentences,\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    speech_batch = self.pad_latent_features(\n",
    "        latent_features,\n",
    "        padding='longest',\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # speech_batch = self.extractor.pad(\n",
    "    #     input_values,\n",
    "    #     padding='longest',\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "    \n",
    "    return speech_batch, text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr/clean (download: 28.05 GiB, generated: 29.10 GiB, post-processed: Unknown size, total: 57.15 GiB) to C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 6.39G/6.39G [16:34<00:00, 6.42MB/s]\n",
      "Downloading data: 100%|██████████| 23.0G/23.0G [1:14:37<00:00, 5.15MB/s]\n",
      "Downloading data files: 100%|██████████| 4/4 [1:31:12<00:00, 1368.10s/it]\n",
      "Extracting data files: 100%|██████████| 4/4 [03:49<00:00, 57.40s/it]\n",
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset librispeech_asr downloaded and prepared to C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  2.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train.100: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 28539\n",
       "    })\n",
       "    train.360: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 104014\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2620\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('librispeech_asr', 'clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104014\n"
     ]
    }
   ],
   "source": [
    "libri = load_dataset('librispeech_asr', 'clean', split='train.360')\n",
    "print(len(libri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2117, 1999, 5197, 2003, 2004, 4076, 12601, 2089, 2022, 4225, 2000, 2022, 1996, 2157, 1997, 2437, 4277, 1999, 2605, 1996, 2332, 2428, 11110, 1037, 4664, 1997, 1996, 11074, 2373, 2144, 1996, 4277, 2031, 2053, 3635, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "test_shard = libri.shard(16, 0)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "test = tokenizer(test_shard[0]['text'], padding='longest', max_length=64, truncation=True, pad_to_max_length=False)\n",
    "print(test)\n",
    "# truncated_shard = test_shard.filter(lambda x: len(x['text']) < 128, num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.12\n"
     ]
    }
   ],
   "source": [
    "print(len(test_shard[0]['audio']['array'])/16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE SECOND IN IMPORTANCE IS AS FOLLOWS SOVEREIGNTY MAY BE DEFINED TO BE THE RIGHT OF MAKING LAWS IN FRANCE THE KING REALLY EXERCISES A PORTION OF THE SOVEREIGN POWER SINCE THE LAWS HAVE NO WEIGHT'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_shard[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6501\n",
      "1133\n"
     ]
    }
   ],
   "source": [
    "print(len(test_shard))\n",
    "print(len(truncated_shard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = {\n",
    "    'input values lenghts': [],\n",
    "    'input ids lenghts': []\n",
    "}\n",
    "for i in range(len(truncated_shard)):\n",
    "    input_lengths['input values lenghts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri = load_dataset('librispeech_asr', 'clean', split='train.360')\n",
    "preprocessor = LibriPreprocessor()\n",
    "\n",
    "num_shards = 16\n",
    "\n",
    "for i in range(num_shards-0):\n",
    "    if i > 10:\n",
    "        print(f\"Creating shard {i}\")\n",
    "        libri_shard = libri.shard(num_shards, i)\n",
    "        libri_shard = libri_shard.map(preprocessor.speech_file_to_array_fn, remove_columns=['file', 'audio', 'speaker_id', 'chapter_id', 'id'])\n",
    "        print(f\"Shard {i} has {len(libri_shard)} items before filtering.\")\n",
    "        libri_filtered = libri_shard.filter(lambda x: len(x['speech'])//x['sampling_rate'] < 16, num_proc=os.cpu_count()) # 16 seconds is the max length of audio\n",
    "        print(f\"Shard {i} has {len(libri_filtered)} items after filtering.\")\n",
    "        libri_prepared = libri_filtered.map(preprocessor.prepare_dataset, batch_size=16, num_proc=1, batched=True, remove_columns=['text', 'sampling_rate'])\n",
    "        # libri_prepared.save_to_disk(f\"../data/librispeech/{i}/\")\n",
    "        libri_prepared.save_to_disk(f\"E:/Machine Learning/Datasets/librispeech/{i}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20526"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libri_filtered_test = libri.filter(lambda x: len(x['audio']['array'])//x['audio']['sampling_rate'] < 10, num_proc=os.cpu_count())\n",
    "len(libri_filtered_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_lengths = []\n",
    "for x in libri:\n",
    "    speech_lengths.append(len(x['audio']['array']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = speech_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_lengths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17040"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(speech_lengths))\n",
    "y = np.array(speech_lengths)\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "wav2vec2 = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = wav2vec2.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(16, 256000, device=device) # 16*256000 worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = extractor(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 499])\n"
     ]
    }
   ],
   "source": [
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    }
   ],
   "source": [
    "libri = load_dataset('librispeech_asr', 'clean', split='train.360')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:\\\\Users\\\\marco\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\d293e31bd75c2251135d0c00f90128737e778f9a133c800b53221a0e82dfde29\\\\1487-133273-0000.flac',\n",
       " 'audio': {'path': 'C:\\\\Users\\\\marco\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\d293e31bd75c2251135d0c00f90128737e778f9a133c800b53221a0e82dfde29\\\\1487-133273-0000.flac',\n",
       "  'array': array([ 9.15527344e-05,  4.57763672e-04,  5.18798828e-04, ...,\n",
       "         -4.57763672e-04, -5.49316406e-04, -4.88281250e-04]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'THE SECOND IN IMPORTANCE IS AS FOLLOWS SOVEREIGNTY MAY BE DEFINED TO BE THE RIGHT OF MAKING LAWS IN FRANCE THE KING REALLY EXERCISES A PORTION OF THE SOVEREIGN POWER SINCE THE LAWS HAVE NO WEIGHT',\n",
       " 'speaker_id': 1487,\n",
       " 'chapter_id': 133273,\n",
       " 'id': '1487-133273-0000'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libri[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['something', 'something else', 'something else again']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(sentences, padding='longest', max_length=64, truncation=True, pad_to_max_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2242, 102, 0, 0], [101, 2242, 2842, 102, 0], [101, 2242, 2842, 2153, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 0, 0], [1, 1, 1, 1, 0], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = [{'input_values': [0.001861572265625, 0.000518798828125, 0.000244140625, 0.0008544921875, 0.001251220703125, 0.0001220703125, -0.0009765625, -0.001556396484375, -0.001800537109375, -0.001953125, -0.001373291015625, 0.0, -0.000640869140625, -0.001434326171875]}, {'input_values': [0.001861572265625, 0.000518798828125, 0.000244140625, 0.0008544921875, 0.001251220703125, 0.0001220703125, -0.0009765625, -0.001556396484375, -0.001800537109375, -0.001953125, -0.001373291015625, 0.0, -0.000640869140625, -0.001434326171875]}, {'input_values': [0.001861572265625, 0.000518798828125, 0.000244140625, 0.0008544921875, 0.001251220703125, 0.0001220703125, -0.0009765625, -0.001434326171875]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_values = extractor.pad(input_values, padding='longest', max_length=32, truncation=True, return_tensors='pt', return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[ 0.0019,  0.0005,  0.0002,  0.0009,  0.0013,  0.0001, -0.0010, -0.0016,\n",
       "         -0.0018, -0.0020, -0.0014,  0.0000, -0.0006, -0.0014],\n",
       "        [ 0.0019,  0.0005,  0.0002,  0.0009,  0.0013,  0.0001, -0.0010, -0.0016,\n",
       "         -0.0018, -0.0020, -0.0014,  0.0000, -0.0006, -0.0014],\n",
       "        [ 0.0019,  0.0005,  0.0002,  0.0009,  0.0013,  0.0001, -0.0010, -0.0014,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2beefd3eff767b677397e49c617fe9eb55de17b3640af7353d98162718fbf92"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('speech-segment-retrieval': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
