{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Union\n",
    "from transformers import BertTokenizerFast, Wav2Vec2FeatureExtractor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriPreprocessor:\n",
    "  def __init__(self):\n",
    "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    self.extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base')\n",
    "    self.feature_encoder = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base').feature_extractor\n",
    "    self.feature_encoder.to(self.device)\n",
    "    self.feature_encoder.eval()\n",
    "  \n",
    "  \n",
    "  def speech_file_to_array_fn(self, data):\n",
    "    #import soundfile as sf\n",
    "    #speech_array, sampling_rate = sf.read(data[\"file\"])\n",
    "    data['speech'] = data['audio']['array']\n",
    "    data['sampling_rate'] = data['audio']['sampling_rate']\n",
    "    #data[\"target_text\"] = data[\"text\"]\n",
    "    return data\n",
    "    \n",
    "    \n",
    "  def prepare_dataset(self, data): \n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(data['sampling_rate'])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {self.extractor.sampling_rate}.\"\n",
    "\n",
    "    # extract and pad input values\n",
    "    input_values = self.extractor(data['speech'], sampling_rate=data['sampling_rate'][0])\n",
    "    data['input_values'] = input_values.input_values\n",
    "    padded_input_values = self.extractor.pad(input_values, return_tensors='pt')\n",
    "    \n",
    "    # compute the latent features from the conv module\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "      input_values = padded_input_values['input_values'].to(self.device)\n",
    "      latent_features = self.feature_encoder(input_values).transpose(1, 2)\n",
    "      latent_features = latent_features.cpu().numpy()\n",
    "      data['latent_features'] = latent_features\n",
    "    \n",
    "    # tokenize text\n",
    "    tokenized_batch = self.tokenizer(data['text'], padding='longest', max_length=128, pad_to_max_length=False)\n",
    "    data['input_ids'] = tokenized_batch['input_ids']\n",
    "    data['attention_mask_text'] = tokenized_batch['attention_mask']\n",
    "    data['token_type_ids_text'] = tokenized_batch['token_type_ids']\n",
    "    \n",
    "    return data\n",
    "  \n",
    "  \n",
    "  def pad_latent_features(self, latent_features, padding='longest', return_tensors=\"pt\"):\n",
    "    padding_value = 0.0\n",
    "    if padding == 'longest':\n",
    "      longest_latent_feature = max(len(item['latent_features']) for item in latent_features)\n",
    "\n",
    "    padded_features = []\n",
    "    for item in latent_features:\n",
    "      latent_features_as_ndarray = np.array(item['latent_features']).astype(np.float32)\n",
    "      padded_item = np.pad(latent_features_as_ndarray, \n",
    "                           ((0, longest_latent_feature - latent_features_as_ndarray.shape[0]), (0, 0)), \n",
    "                           mode='constant', \n",
    "                           constant_values=padding_value)\n",
    "      if return_tensors == \"pt\":\n",
    "        padded_item = torch.from_numpy(padded_item).to(torch.float32)\n",
    "      padded_features.append(padded_item)\n",
    "      \n",
    "    if return_tensors == \"pt\":\n",
    "      padded_features = torch.stack(padded_features)\n",
    "      \n",
    "    return padded_features\n",
    "\n",
    "\n",
    "  def __call__(\n",
    "    self,\n",
    "    batch: List[Dict[str, Union[List[int], torch.Tensor]]],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function to be used when training with PyTorch Lightning.\n",
    "    Returns:\n",
    "        :obj:`Dict[str, torch.Tensor]`: A dictionary of tensors containing the collated features.\n",
    "    \"\"\" \n",
    "    latent_features = [{'latent_features': feature['latent_features']} for feature in batch]\n",
    "    # input_values = [{'input_values': feature['input_values']} for feature in batch]\n",
    "    input_sentences = [{'input_ids': feature['input_ids']} for feature in batch]\n",
    "    \n",
    "    text_batch = self.tokenizer.pad(\n",
    "        input_sentences,\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    speech_batch = self.pad_latent_features(\n",
    "        latent_features,\n",
    "        padding='longest',\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # speech_batch = self.extractor.pad(\n",
    "    #     input_values,\n",
    "    #     padding='longest',\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "    \n",
    "    return speech_batch, text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr/clean (download: 28.05 GiB, generated: 29.10 GiB, post-processed: Unknown size, total: 57.15 GiB) to C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 6.39G/6.39G [16:34<00:00, 6.42MB/s]\n",
      "Downloading data: 100%|██████████| 23.0G/23.0G [1:14:37<00:00, 5.15MB/s]\n",
      "Downloading data files: 100%|██████████| 4/4 [1:31:12<00:00, 1368.10s/it]\n",
      "Extracting data files: 100%|██████████| 4/4 [03:49<00:00, 57.40s/it]\n",
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset librispeech_asr downloaded and prepared to C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  2.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train.100: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 28539\n",
       "    })\n",
       "    train.360: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 104014\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2620\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('librispeech_asr', 'clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104014\n"
     ]
    }
   ],
   "source": [
    "libri = load_dataset('librispeech_asr', 'clean', split='train.360')\n",
    "print(len(libri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2117, 1999, 5197, 2003, 2004, 4076, 12601, 2089, 2022, 4225, 2000, 2022, 1996, 2157, 1997, 2437, 4277, 1999, 2605, 1996, 2332, 2428, 11110, 1037, 4664, 1997, 1996, 11074, 2373, 2144, 1996, 4277, 2031, 2053, 3635, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "test_shard = libri.shard(16, 0)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "test = tokenizer(test_shard[0]['text'], padding='longest', max_length=64, truncation=True, pad_to_max_length=False)\n",
    "print(test)\n",
    "# truncated_shard = test_shard.filter(lambda x: len(x['text']) < 128, num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.12\n"
     ]
    }
   ],
   "source": [
    "print(len(test_shard[0]['audio']['array'])/16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE SECOND IN IMPORTANCE IS AS FOLLOWS SOVEREIGNTY MAY BE DEFINED TO BE THE RIGHT OF MAKING LAWS IN FRANCE THE KING REALLY EXERCISES A PORTION OF THE SOVEREIGN POWER SINCE THE LAWS HAVE NO WEIGHT'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_shard[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6501\n",
      "1133\n"
     ]
    }
   ],
   "source": [
    "print(len(test_shard))\n",
    "print(len(truncated_shard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = {\n",
    "    'input values lenghts': [],\n",
    "    'input ids lenghts': []\n",
    "}\n",
    "for i in range(len(truncated_shard)):\n",
    "    input_lengths['input values lenghts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "c:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.bias', 'project_q.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shard 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6501/6501 [01:00<00:00, 108.23ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 5 has 6501 items before filtering.\n",
      "Shard 5 has 6097 items after filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/382 [00:00<?, ?ba/s]c:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\transformers\\feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      "100%|██████████| 382/382 [15:05<00:00,  2.37s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shard 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6501/6501 [01:01<00:00, 105.73ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 6 has 6501 items before filtering.\n",
      "Shard 6 has 6089 items after filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [14:16<00:00,  2.25s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shard 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6501/6501 [00:59<00:00, 109.42ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 7 has 6501 items before filtering.\n",
      "Shard 7 has 6133 items after filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [14:21<00:00,  2.24s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shard 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6501/6501 [00:59<00:00, 109.17ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 8 has 6501 items before filtering.\n",
      "Shard 8 has 6090 items after filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [13:37<00:00,  2.14s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shard 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6501/6501 [01:01<00:00, 105.84ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 9 has 6501 items before filtering.\n",
      "Shard 9 has 6113 items after filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [14:12<00:00,  2.23s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shard 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6501/6501 [00:59<00:00, 109.11ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 10 has 6501 items before filtering.\n",
      "Shard 10 has 6081 items after filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [13:59<00:00,  2.20s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shard 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1999/6501 [00:17<00:39, 113.46ex/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\Cross-Modal Speech Segment Retrieval\\cross-modal-speech-segment-retrieval\\notebooks\\01-mm-librispeech_preprocessing.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/Cross-Modal%20Speech%20Segment%20Retrieval/cross-modal-speech-segment-retrieval/notebooks/01-mm-librispeech_preprocessing.ipynb#ch0000004?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCreating shard \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/Cross-Modal%20Speech%20Segment%20Retrieval/cross-modal-speech-segment-retrieval/notebooks/01-mm-librispeech_preprocessing.ipynb#ch0000004?line=8'>9</a>\u001b[0m libri_shard \u001b[39m=\u001b[39m libri\u001b[39m.\u001b[39mshard(num_shards, i)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/Cross-Modal%20Speech%20Segment%20Retrieval/cross-modal-speech-segment-retrieval/notebooks/01-mm-librispeech_preprocessing.ipynb#ch0000004?line=9'>10</a>\u001b[0m libri_shard \u001b[39m=\u001b[39m libri_shard\u001b[39m.\u001b[39;49mmap(preprocessor\u001b[39m.\u001b[39;49mspeech_file_to_array_fn, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mfile\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mspeaker_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mchapter_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/Cross-Modal%20Speech%20Segment%20Retrieval/cross-modal-speech-segment-retrieval/notebooks/01-mm-librispeech_preprocessing.ipynb#ch0000004?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShard \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m has \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(libri_shard)\u001b[39m}\u001b[39;00m\u001b[39m items before filtering.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/Cross-Modal%20Speech%20Segment%20Retrieval/cross-modal-speech-segment-retrieval/notebooks/01-mm-librispeech_preprocessing.ipynb#ch0000004?line=11'>12</a>\u001b[0m libri_filtered \u001b[39m=\u001b[39m libri_shard\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x[\u001b[39m'\u001b[39m\u001b[39mspeech\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mx[\u001b[39m'\u001b[39m\u001b[39msampling_rate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m16\u001b[39m, num_proc\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mcpu_count()) \u001b[39m# 16 seconds is the max length of audio\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_dataset.py:2346\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2342'>2343</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2344'>2345</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2345'>2346</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2346'>2347</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2347'>2348</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2348'>2349</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2349'>2350</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2350'>2351</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2351'>2352</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2352'>2353</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2353'>2354</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2354'>2355</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2355'>2356</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2356'>2357</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2357'>2358</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2358'>2359</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2359'>2360</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2360'>2361</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2361'>2362</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2362'>2363</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2363'>2364</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2364'>2365</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2365'>2366</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2367'>2368</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_dataset.py:532\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=529'>530</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=530'>531</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=531'>532</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=532'>533</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=533'>534</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=534'>535</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_dataset.py:499\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=491'>492</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=492'>493</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=493'>494</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=494'>495</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=495'>496</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=496'>497</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=497'>498</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=498'>499</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=499'>500</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=500'>501</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/fingerprint.py?line=451'>452</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/fingerprint.py?line=452'>453</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/fingerprint.py?line=453'>454</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/fingerprint.py?line=455'>456</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/fingerprint.py?line=457'>458</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/fingerprint.py?line=459'>460</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/fingerprint.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_dataset.py:2723\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2720'>2721</a>\u001b[0m                 writer\u001b[39m.\u001b[39mwrite_row(example)\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2721'>2722</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2722'>2723</a>\u001b[0m                 writer\u001b[39m.\u001b[39;49mwrite(example)\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2723'>2724</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_dataset.py?line=2724'>2725</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pbar:\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_writer.py:456\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[1;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=452'>453</a>\u001b[0m     \u001b[39m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=453'>454</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[1;32m--> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=455'>456</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_writer.py:414\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=410'>411</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m cols:\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=411'>412</a>\u001b[0m     \u001b[39m# Since current_examples contains (example, key) tuples\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=412'>413</a>\u001b[0m     batch_examples[col] \u001b[39m=\u001b[39m [row[\u001b[39m0\u001b[39m][col] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples]\n\u001b[1;32m--> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=413'>414</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=414'>415</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_writer.py:507\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=504'>505</a>\u001b[0m     col_try_type \u001b[39m=\u001b[39m try_features[col] \u001b[39mif\u001b[39;00m try_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m col \u001b[39min\u001b[39;00m try_features \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=505'>506</a>\u001b[0m     typed_sequence \u001b[39m=\u001b[39m OptimizedTypedSequence(batch_examples[col], \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mcol_type, try_type\u001b[39m=\u001b[39mcol_try_type, col\u001b[39m=\u001b[39mcol)\n\u001b[1;32m--> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=506'>507</a>\u001b[0m     arrays\u001b[39m.\u001b[39mappend(pa\u001b[39m.\u001b[39;49marray(typed_sequence))\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=507'>508</a>\u001b[0m     inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=508'>509</a>\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\pyarrow\\array.pxi:229\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\pyarrow\\array.pxi:110\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\arrow_writer.py:184\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=181'>182</a>\u001b[0m     out \u001b[39m=\u001b[39m numpy_to_pyarrow_listarray(data)\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=182'>183</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(first_non_null_value(data)[\u001b[39m1\u001b[39m], np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m--> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=183'>184</a>\u001b[0m     out \u001b[39m=\u001b[39m list_of_np_array_to_pyarrow_listarray(data)\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=184'>185</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/arrow_writer.py?line=185'>186</a>\u001b[0m     trying_cast_to_python_objects \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\features\\features.py:1193\u001b[0m, in \u001b[0;36mlist_of_np_array_to_pyarrow_listarray\u001b[1;34m(l_arr, type)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1189'>1190</a>\u001b[0m \u001b[39m\"\"\"Build a PyArrow ListArray from a possibly nested list of NumPy arrays\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1190'>1191</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(l_arr) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1191'>1192</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m list_of_pa_arrays_to_pyarrow_listarray(\n\u001b[1;32m-> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1192'>1193</a>\u001b[0m         [numpy_to_pyarrow_listarray(arr, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m) \u001b[39mif\u001b[39;00m arr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m l_arr]\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1193'>1194</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1194'>1195</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1195'>1196</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39marray([], \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\features\\features.py:1193\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1189'>1190</a>\u001b[0m \u001b[39m\"\"\"Build a PyArrow ListArray from a possibly nested list of NumPy arrays\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1190'>1191</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(l_arr) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1191'>1192</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m list_of_pa_arrays_to_pyarrow_listarray(\n\u001b[1;32m-> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1192'>1193</a>\u001b[0m         [numpy_to_pyarrow_listarray(arr, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mtype\u001b[39;49m) \u001b[39mif\u001b[39;00m arr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m l_arr]\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1193'>1194</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1194'>1195</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1195'>1196</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39marray([], \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\datasets\\features\\features.py:1168\u001b[0m, in \u001b[0;36mnumpy_to_pyarrow_listarray\u001b[1;34m(arr, type)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1165'>1166</a>\u001b[0m \u001b[39m\"\"\"Build a PyArrow ListArray from a multidimensional NumPy array\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1166'>1167</a>\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(arr)\n\u001b[1;32m-> <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1167'>1168</a>\u001b[0m values \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39marray(arr\u001b[39m.\u001b[39;49mflatten(), \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1168'>1169</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(arr\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/marco/Google%20Drive/Projects/.venv/speech-segment-retrieval/lib/site-packages/datasets/features/features.py?line=1169'>1170</a>\u001b[0m     n_offsets \u001b[39m=\u001b[39m reduce(mul, arr\u001b[39m.\u001b[39mshape[: arr\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "libri = load_dataset('librispeech_asr', 'clean', split='train.360')\n",
    "preprocessor = LibriPreprocessor()\n",
    "\n",
    "num_shards = 16\n",
    "\n",
    "for i in range(num_shards-0):\n",
    "    if i > 10:\n",
    "        print(f\"Creating shard {i}\")\n",
    "        libri_shard = libri.shard(num_shards, i)\n",
    "        libri_shard = libri_shard.map(preprocessor.speech_file_to_array_fn, remove_columns=['file', 'audio', 'speaker_id', 'chapter_id', 'id'])\n",
    "        print(f\"Shard {i} has {len(libri_shard)} items before filtering.\")\n",
    "        libri_filtered = libri_shard.filter(lambda x: len(x['speech'])//x['sampling_rate'] < 16, num_proc=os.cpu_count()) # 16 seconds is the max length of audio\n",
    "        print(f\"Shard {i} has {len(libri_filtered)} items after filtering.\")\n",
    "        libri_prepared = libri_filtered.map(preprocessor.prepare_dataset, batch_size=16, num_proc=1, batched=True, remove_columns=['text', 'sampling_rate'])\n",
    "        # libri_prepared.save_to_disk(f\"../data/librispeech/{i}/\")\n",
    "        libri_prepared.save_to_disk(f\"E:/Machine Learning/Datasets/librispeech/{i}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20526"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libri_filtered_test = libri.filter(lambda x: len(x['audio']['array'])//x['audio']['sampling_rate'] < 10, num_proc=os.cpu_count())\n",
    "len(libri_filtered_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_lengths = []\n",
    "for x in libri:\n",
    "    speech_lengths.append(len(x['audio']['array']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = speech_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_lengths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17040"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(speech_lengths))\n",
    "y = np.array(speech_lengths)\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\Google Drive\\Projects\\.venv\\speech-segment-retrieval\\lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "wav2vec2 = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = wav2vec2.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(16, 256000, device=device) # 16*256000 worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = extractor(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 499])\n"
     ]
    }
   ],
   "source": [
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (C:\\Users\\marco\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    }
   ],
   "source": [
    "libri = load_dataset('librispeech_asr', 'clean', split='train.360')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:\\\\Users\\\\marco\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\d293e31bd75c2251135d0c00f90128737e778f9a133c800b53221a0e82dfde29\\\\1487-133273-0000.flac',\n",
       " 'audio': {'path': 'C:\\\\Users\\\\marco\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\d293e31bd75c2251135d0c00f90128737e778f9a133c800b53221a0e82dfde29\\\\1487-133273-0000.flac',\n",
       "  'array': array([ 9.15527344e-05,  4.57763672e-04,  5.18798828e-04, ...,\n",
       "         -4.57763672e-04, -5.49316406e-04, -4.88281250e-04]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'THE SECOND IN IMPORTANCE IS AS FOLLOWS SOVEREIGNTY MAY BE DEFINED TO BE THE RIGHT OF MAKING LAWS IN FRANCE THE KING REALLY EXERCISES A PORTION OF THE SOVEREIGN POWER SINCE THE LAWS HAVE NO WEIGHT',\n",
       " 'speaker_id': 1487,\n",
       " 'chapter_id': 133273,\n",
       " 'id': '1487-133273-0000'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libri[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['something', 'something else', 'something else again']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(sentences, padding='longest', max_length=64, truncation=True, pad_to_max_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2242, 102, 0, 0], [101, 2242, 2842, 102, 0], [101, 2242, 2842, 2153, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 0, 0], [1, 1, 1, 1, 0], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = [{'input_values': [0.001861572265625, 0.000518798828125, 0.000244140625, 0.0008544921875, 0.001251220703125, 0.0001220703125, -0.0009765625, -0.001556396484375, -0.001800537109375, -0.001953125, -0.001373291015625, 0.0, -0.000640869140625, -0.001434326171875]}, {'input_values': [0.001861572265625, 0.000518798828125, 0.000244140625, 0.0008544921875, 0.001251220703125, 0.0001220703125, -0.0009765625, -0.001556396484375, -0.001800537109375, -0.001953125, -0.001373291015625, 0.0, -0.000640869140625, -0.001434326171875]}, {'input_values': [0.001861572265625, 0.000518798828125, 0.000244140625, 0.0008544921875, 0.001251220703125, 0.0001220703125, -0.0009765625, -0.001434326171875]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_values = extractor.pad(input_values, padding='longest', max_length=32, truncation=True, return_tensors='pt', return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[ 0.0019,  0.0005,  0.0002,  0.0009,  0.0013,  0.0001, -0.0010, -0.0016,\n",
       "         -0.0018, -0.0020, -0.0014,  0.0000, -0.0006, -0.0014],\n",
       "        [ 0.0019,  0.0005,  0.0002,  0.0009,  0.0013,  0.0001, -0.0010, -0.0016,\n",
       "         -0.0018, -0.0020, -0.0014,  0.0000, -0.0006, -0.0014],\n",
       "        [ 0.0019,  0.0005,  0.0002,  0.0009,  0.0013,  0.0001, -0.0010, -0.0014,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2beefd3eff767b677397e49c617fe9eb55de17b3640af7353d98162718fbf92"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('speech-segment-retrieval': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
