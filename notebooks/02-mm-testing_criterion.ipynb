{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pooler_output: torch.Tensor,\n",
    "        last_hidden_state: torch.Tensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attentions: torch.Tensor,\n",
    "        cross_attentions: torch.Tensor    \n",
    "    ) -> None:\n",
    "        self.pooler_output = pooler_output\n",
    "        self.last_hidden_state = last_hidden_state\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attentions = attentions\n",
    "        self.cross_attentions = cross_attentions\n",
    "\n",
    "class ForwardPassOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        student_output = None,\n",
    "        teacher_output = None,\n",
    "        align_fuse: dict = None,\n",
    "        labels: torch.Tensor = None,\n",
    "        output_modalities: dict = None\n",
    "    ) -> None:\n",
    "        self.student_output = student_output\n",
    "        self.teacher_output = teacher_output\n",
    "        self.align_fuse = align_fuse\n",
    "        self.labels = labels\n",
    "        self.output_modalities = output_modalities\n",
    "        \n",
    "    def set_attributes(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_output = ModelOutput(pooler_output=torch.randn(64,128), last_hidden_state=torch.randn(64,256,128), hidden_states=[torch.randn(64,256,128)]*8, attentions=[torch.randn(64,8,256,256)]*8, cross_attentions=[torch.randn(64,1,256,299)])\n",
    "teacher_output = ModelOutput(pooler_output=torch.randn(64,128), last_hidden_state=torch.randn(64,256,128), hidden_states=[torch.randn(64,256,128)]*8, attentions=[torch.randn(64,8,256,256)]*8, cross_attentions=[torch.randn(64,1,256,299)])\n",
    "outputs = ForwardPassOutput(student_output=student_output, teacher_output=teacher_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.baeldung.com/cs/instance-vs-batch-normalization\n",
    "\n",
    "class LatentPredictionLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden_layers_to_predict: int,\n",
    "        reduction: str = \"mean\",\n",
    "        beta: float = 1.0        \n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction=reduction, beta=beta)\n",
    "        \n",
    "        self.num_hidden_layers_to_predict = num_hidden_layers_to_predict\n",
    "        \n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        fwd_output: ForwardPassOutput,\n",
    "        ) -> torch.Tensor:\n",
    "        \n",
    "        # take the last transformer layers from the student\n",
    "        x = fwd_output.student_output.hidden_states[-1:][0]\n",
    "        # Follow the same layer normalization for all modalities\n",
    "        x = [torch.layer_norm(tl.float(), tl.shape[-1:]) for tl in x]\n",
    "        x = sum(x) / len(x)\n",
    "        # normalize targets\n",
    "        x = torch.layer_norm(x.float(), x.shape[-1:])\n",
    "    \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # take the last k transformer layers from the teacher\n",
    "            y = fwd_output.teacher_output.hidden_states[-self.num_hidden_layers_to_predict:]\n",
    "            # Follow the same layer normalization for all modalities\n",
    "            y = [torch.layer_norm(tl.float(), tl.shape[-1:]) for tl in y]\n",
    "            y = sum(y) / len(y)\n",
    "            # normalize targets\n",
    "            y = torch.layer_norm(y.float(), y.shape[-1:])\n",
    "                \n",
    "        hidden_states_loss = self.loss_fn(x, y)\n",
    "        \n",
    "        x_pooler = fwd_output.student_output.pooler_output\n",
    "        y_pooler = fwd_output.teacher_output.pooler_output\n",
    "        pooler_loss = self.loss_fn(x_pooler, y_pooler) \n",
    "        \n",
    "        loss = hidden_states_loss + pooler_loss\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\torch\\nn\\modules\\loss.py:922: UserWarning: Using a target size (torch.Size([64, 256, 128])) that is different to the input size (torch.Size([256, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1134, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 311, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"c:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2062, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"c:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2098, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m loss \u001b[39m=\u001b[39m LatentPredictionLoss(num_hidden_layers_to_predict\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m loss(outputs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [33], line 49\u001b[0m, in \u001b[0;36mLatentPredictionLoss.forward\u001b[1;34m(self, fwd_output)\u001b[0m\n\u001b[0;32m     45\u001b[0m pooler_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(x_pooler, y_pooler) \n\u001b[0;32m     47\u001b[0m loss \u001b[39m=\u001b[39m hidden_states_loss \u001b[39m+\u001b[39m pooler_loss\n\u001b[1;32m---> 49\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "Cell \u001b[1;32mIn [33], line 49\u001b[0m, in \u001b[0;36mLatentPredictionLoss.forward\u001b[1;34m(self, fwd_output)\u001b[0m\n\u001b[0;32m     45\u001b[0m pooler_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(x_pooler, y_pooler) \n\u001b[0;32m     47\u001b[0m loss \u001b[39m=\u001b[39m hidden_states_loss \u001b[39m+\u001b[39m pooler_loss\n\u001b[1;32m---> 49\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1443\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:700\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1143\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1134\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2059\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2061\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 2062\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2064\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2067\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\.venv\\multimodal-ssl\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2095\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2098\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2102\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = LatentPredictionLoss(num_hidden_layers_to_predict=2)\n",
    "loss(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = torch.nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('multimodal-ssl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ac8a2bc69d4b2bdc42aaccd63f192d886c476dacd93adfa548f17911c905576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
