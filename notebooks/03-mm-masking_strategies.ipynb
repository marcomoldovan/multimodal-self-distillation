{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\Google Drive\\Projects\\.venv\\perceiver-data2vec\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = torch.randn(1, 52097, 704)\n",
    "latent_input = torch.randn(1, 784, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mask_indices(\n",
    "    shape: Tuple[int, int],\n",
    "    mask_prob: float,\n",
    "    mask_length: int,\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    min_masks: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n",
    "    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n",
    "    CPU as part of the preprocessing during training.\n",
    "    Args:\n",
    "        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n",
    "               the first element is the batch size and the second element is the length of the axis to span.\n",
    "        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n",
    "                    independently generated mask spans of length `mask_length` is computed by\n",
    "                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n",
    "                    actual percentage will be smaller.\n",
    "        mask_length: size of the mask\n",
    "        min_masks: minimum number of masked spans\n",
    "        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n",
    "                        each batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size, sequence_length = shape\n",
    "\n",
    "    if mask_length < 1:\n",
    "        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n",
    "\n",
    "    if mask_length > sequence_length:\n",
    "        raise ValueError(\n",
    "            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n",
    "            f\" and `sequence_length`: {sequence_length}`\"\n",
    "        )\n",
    "\n",
    "    # epsilon is used for probabilistic rounding\n",
    "    epsilon = np.random.rand(1).item()\n",
    "\n",
    "    def compute_num_masked_span(input_length):\n",
    "        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n",
    "        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n",
    "        num_masked_span = max(num_masked_span, min_masks)\n",
    "\n",
    "        # make sure num masked span <= sequence_length\n",
    "        if num_masked_span * mask_length > sequence_length:\n",
    "            num_masked_span = sequence_length // mask_length\n",
    "\n",
    "        # make sure num_masked span is also <= input_length - (mask_length - 1)\n",
    "        if input_length - (mask_length - 1) < num_masked_span:\n",
    "            num_masked_span = max(input_length - (mask_length - 1), 0)\n",
    "\n",
    "        return num_masked_span\n",
    "\n",
    "    # compute number of masked spans in batch\n",
    "    input_lengths = (\n",
    "        attention_mask.sum(-1).detach().tolist()\n",
    "        if attention_mask is not None\n",
    "        else [sequence_length for _ in range(batch_size)]\n",
    "    )\n",
    "\n",
    "    # SpecAugment mask to fill\n",
    "    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n",
    "    spec_aug_mask_idxs = []\n",
    "\n",
    "    max_num_masked_span = compute_num_masked_span(sequence_length)\n",
    "\n",
    "    if max_num_masked_span == 0:\n",
    "        return spec_aug_mask\n",
    "\n",
    "    for input_length in input_lengths:\n",
    "        # compute num of masked spans for this input\n",
    "        num_masked_span = compute_num_masked_span(input_length)\n",
    "\n",
    "        # get random indices to mask\n",
    "        spec_aug_mask_idx = np.random.choice(\n",
    "            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n",
    "        )\n",
    "\n",
    "        # pick first sampled index that will serve as a dummy index to pad vector\n",
    "        # to ensure same dimension for all batches due to probabilistic rounding\n",
    "        # Picking first sample just pads those vectors twice.\n",
    "        if len(spec_aug_mask_idx) == 0:\n",
    "            # this case can only happen if `input_length` is strictly smaller then\n",
    "            # `sequence_length` in which case the last token has to be a padding\n",
    "            # token which we can use as a dummy mask id\n",
    "            dummy_mask_idx = sequence_length - 1\n",
    "        else:\n",
    "            dummy_mask_idx = spec_aug_mask_idx[0]\n",
    "\n",
    "        spec_aug_mask_idx = np.concatenate(\n",
    "            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n",
    "        )\n",
    "        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n",
    "\n",
    "    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n",
    "\n",
    "    # expand masked indices to masked spans\n",
    "    spec_aug_mask_idxs = np.broadcast_to(\n",
    "        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n",
    "    )\n",
    "    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n",
    "\n",
    "    # add offset to the starting indexes so that that indexes now create a span\n",
    "    offsets = np.arange(mask_length)[None, None, :]\n",
    "    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n",
    "        batch_size, max_num_masked_span * mask_length\n",
    "    )\n",
    "    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n",
    "\n",
    "    # ensure that we cannot have indices larger than sequence_length\n",
    "    if spec_aug_mask_idxs.max() > sequence_length - 1:\n",
    "        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n",
    "\n",
    "    # scatter indices to mask\n",
    "    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n",
    "\n",
    "    return spec_aug_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_hidden_states(\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        mask_time_indices: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        mask_time_prob: float = 0.05,\n",
    "        mask_time_length: int = 10,\n",
    "        mask_feature_prob: float = 0.0,\n",
    "        mask_feature_length: int = 10,\n",
    "        min_masks: int = 0,\n",
    "        training: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Masks extracted features along time axis and/or along feature axis according to\n",
    "        [SpecAugment](https://arxiv.org/abs/1904.08779).\n",
    "        \"\"\"\n",
    "        _, _, hidden_size = hidden_states.size()\n",
    "\n",
    "        if mask_time_prob > 0.0 or mask_feature_prob > 0.0:\n",
    "            masked_spec_embed = torch.nn.Parameter(torch.FloatTensor(hidden_size).uniform_())\n",
    "\n",
    "        # generate indices & apply SpecAugment along time axis\n",
    "        batch_size, sequence_length, hidden_size = hidden_states.size()\n",
    "\n",
    "        if mask_time_indices is not None:\n",
    "            # apply SpecAugment along time axis with given mask_time_indices\n",
    "            hidden_states[mask_time_indices] = masked_spec_embed.to(hidden_states.dtype)\n",
    "        elif mask_time_prob > 0 and training:\n",
    "            mask_time_indices = _compute_mask_indices(\n",
    "                (batch_size, sequence_length),\n",
    "                mask_prob=mask_time_prob,\n",
    "                mask_length=mask_time_length,\n",
    "                attention_mask=attention_mask,\n",
    "                min_masks=min_masks,\n",
    "            )\n",
    "            mask_time_indices = torch.tensor(mask_time_indices, device=hidden_states.device, dtype=torch.bool)\n",
    "            hidden_states[mask_time_indices] = masked_spec_embed.to(hidden_states.dtype)\n",
    "\n",
    "        if mask_feature_prob > 0 and training:\n",
    "            # generate indices & apply SpecAugment along feature axis\n",
    "            mask_feature_indices = _compute_mask_indices(\n",
    "                (batch_size, hidden_size),\n",
    "                mask_prob=mask_feature_prob,\n",
    "                mask_length=mask_feature_length,\n",
    "                min_masks=min_masks,\n",
    "            )\n",
    "            mask_feature_indices = torch.tensor(mask_feature_indices, device=hidden_states.device, dtype=torch.bool)\n",
    "            mask_feature_indices = mask_feature_indices[:, None].expand(-1, sequence_length, -1)\n",
    "            hidden_states[mask_feature_indices] = 0\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "input_states = torch.randn(32, 1024, 128)\n",
    "mask_indices = _compute_mask_indices((32, 1024), 0.15, 10)\n",
    "print(mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_input_states = _mask_hidden_states(input_states, mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8172,  0.4247,  0.6360,  ..., -0.4846, -0.3938,  0.8664],\n",
       "         [ 0.3479, -0.0274,  1.4342,  ..., -0.4928, -0.5790,  0.9007],\n",
       "         [ 0.7716, -0.5302, -0.5852,  ..., -0.4640,  0.3098, -0.5471],\n",
       "         ...,\n",
       "         [-0.5887,  0.2390,  1.2929,  ...,  0.0108, -1.5793,  1.2905],\n",
       "         [-1.2090,  1.1563, -0.8621,  ...,  1.6436,  2.1626, -0.1252],\n",
       "         [-0.3605,  0.3523,  0.9850,  ...,  1.5247, -0.7886, -0.5023]],\n",
       "\n",
       "        [[ 0.1422, -1.8492, -0.1127,  ..., -0.5773, -0.8135, -0.1632],\n",
       "         [ 0.2366,  1.8626,  0.1292,  ...,  0.5965,  0.1556,  0.8070],\n",
       "         [ 0.0761,  0.7843, -0.2418,  ..., -0.5750, -1.5600,  0.1633],\n",
       "         ...,\n",
       "         [-0.0630, -0.8160, -0.2753,  ..., -0.7918,  2.0991, -0.8249],\n",
       "         [ 0.3926,  0.8123, -1.4953,  ..., -0.6572, -1.0552, -0.9915],\n",
       "         [ 0.8568, -1.3576, -0.9127,  ...,  0.1001,  2.1232,  1.1134]],\n",
       "\n",
       "        [[-1.7675,  0.1361,  1.7224,  ...,  2.2313,  0.5240, -0.3489],\n",
       "         [ 0.8673, -1.1345, -0.5816,  ...,  1.2042, -1.5318,  2.2152],\n",
       "         [-0.0141,  1.3871,  0.2036,  ..., -3.2694,  1.1599, -0.8414],\n",
       "         ...,\n",
       "         [ 0.4995, -0.5364,  0.5199,  ...,  1.4861, -2.2520,  0.5391],\n",
       "         [ 0.3897,  0.8024,  0.4558,  ...,  2.2413, -0.3542,  1.5330],\n",
       "         [ 0.4824,  1.7755,  0.4016,  ..., -1.9431, -0.6306, -0.0577]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0658,  0.9800,  0.1445,  ...,  0.5769, -0.8901, -0.3978],\n",
       "         [-0.7948,  1.4999, -1.5691,  ..., -1.3308,  0.3994, -2.5434],\n",
       "         [ 0.7747,  1.9239,  1.1527,  ...,  0.3123,  0.6671,  1.6440],\n",
       "         ...,\n",
       "         [-0.0146, -0.0488, -0.4944,  ..., -1.5805,  2.0690, -1.7729],\n",
       "         [ 1.3905,  0.3141, -0.3228,  ...,  0.2223,  0.0562,  1.6036],\n",
       "         [ 0.2828, -0.4854, -0.8413,  ...,  0.4942, -0.5399,  0.0603]],\n",
       "\n",
       "        [[ 1.3959, -0.2697, -0.5095,  ..., -0.5687, -0.3020, -1.6685],\n",
       "         [ 0.1434,  0.3525,  0.0274,  ...,  0.9432,  1.7079, -2.1668],\n",
       "         [-1.5521, -0.8244, -1.1191,  ..., -1.2542, -0.7270, -0.7815],\n",
       "         ...,\n",
       "         [-1.1064, -0.5524,  0.1682,  ...,  2.0744, -1.0026, -0.4883],\n",
       "         [-0.1466,  0.6700,  0.3859,  ...,  1.4566,  0.2925, -0.5683],\n",
       "         [ 0.0733, -1.6852,  1.7543,  ...,  0.0880,  0.7795,  0.4289]],\n",
       "\n",
       "        [[ 0.6936, -0.7191,  0.5027,  ..., -1.2806, -0.2858, -1.3260],\n",
       "         [ 1.2644,  1.1038, -0.4895,  ...,  0.9795, -0.9902,  1.2165],\n",
       "         [ 0.3521,  0.3725,  0.6693,  ..., -1.3563,  0.4210,  0.9323],\n",
       "         ...,\n",
       "         [-0.6495, -0.1284,  0.9049,  ..., -0.2460, -0.7954, -1.4673],\n",
       "         [-1.1696, -0.4767, -1.7277,  ...,  0.2574,  0.2871,  0.6998],\n",
       "         [-0.0408,  0.3401,  0.4557,  ...,  0.9000, -0.2136,  0.7213]]],\n",
       "       grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('perceiver-data2vec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "356f0f1e9d918dde982ecf27c70cbfdd6ae28858b3c646ccfe6075c66f643012"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
