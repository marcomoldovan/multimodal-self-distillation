{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = torch.randn(1, 52097, 704)\n",
    "latent_input = torch.randn(1, 784, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mask_indices(\n",
    "    shape: Tuple[int, int],\n",
    "    mask_prob: float,\n",
    "    mask_length: int,\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    min_masks: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n",
    "    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n",
    "    CPU as part of the preprocessing during training.\n",
    "    Args:\n",
    "        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n",
    "               the first element is the batch size and the second element is the length of the axis to span.\n",
    "        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n",
    "                    independently generated mask spans of length `mask_length` is computed by\n",
    "                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n",
    "                    actual percentage will be smaller.\n",
    "        mask_length: size of the mask\n",
    "        min_masks: minimum number of masked spans\n",
    "        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n",
    "                        each batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size, sequence_length = shape\n",
    "\n",
    "    if mask_length < 1:\n",
    "        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n",
    "\n",
    "    if mask_length > sequence_length:\n",
    "        raise ValueError(\n",
    "            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n",
    "            f\" and `sequence_length`: {sequence_length}`\"\n",
    "        )\n",
    "\n",
    "    # epsilon is used for probabilistic rounding\n",
    "    epsilon = np.random.rand(1).item()\n",
    "\n",
    "    def compute_num_masked_span(input_length):\n",
    "        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n",
    "        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n",
    "        num_masked_span = max(num_masked_span, min_masks)\n",
    "\n",
    "        # make sure num masked span <= sequence_length\n",
    "        if num_masked_span * mask_length > sequence_length:\n",
    "            num_masked_span = sequence_length // mask_length\n",
    "\n",
    "        # make sure num_masked span is also <= input_length - (mask_length - 1)\n",
    "        if input_length - (mask_length - 1) < num_masked_span:\n",
    "            num_masked_span = max(input_length - (mask_length - 1), 0)\n",
    "\n",
    "        return num_masked_span\n",
    "\n",
    "    # compute number of masked spans in batch\n",
    "    input_lengths = (\n",
    "        attention_mask.sum(-1).detach().tolist()\n",
    "        if attention_mask is not None\n",
    "        else [sequence_length for _ in range(batch_size)]\n",
    "    )\n",
    "\n",
    "    # SpecAugment mask to fill\n",
    "    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n",
    "    spec_aug_mask_idxs = []\n",
    "\n",
    "    max_num_masked_span = compute_num_masked_span(sequence_length)\n",
    "\n",
    "    if max_num_masked_span == 0:\n",
    "        return spec_aug_mask\n",
    "\n",
    "    for input_length in input_lengths:\n",
    "        # compute num of masked spans for this input\n",
    "        num_masked_span = compute_num_masked_span(input_length)\n",
    "\n",
    "        # get random indices to mask\n",
    "        spec_aug_mask_idx = np.random.choice(\n",
    "            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n",
    "        )\n",
    "\n",
    "        # pick first sampled index that will serve as a dummy index to pad vector\n",
    "        # to ensure same dimension for all batches due to probabilistic rounding\n",
    "        # Picking first sample just pads those vectors twice.\n",
    "        if len(spec_aug_mask_idx) == 0:\n",
    "            # this case can only happen if `input_length` is strictly smaller then\n",
    "            # `sequence_length` in which case the last token has to be a padding\n",
    "            # token which we can use as a dummy mask id\n",
    "            dummy_mask_idx = sequence_length - 1\n",
    "        else:\n",
    "            dummy_mask_idx = spec_aug_mask_idx[0]\n",
    "\n",
    "        spec_aug_mask_idx = np.concatenate(\n",
    "            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n",
    "        )\n",
    "        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n",
    "\n",
    "    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n",
    "\n",
    "    # expand masked indices to masked spans\n",
    "    spec_aug_mask_idxs = np.broadcast_to(\n",
    "        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n",
    "    )\n",
    "    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n",
    "\n",
    "    # add offset to the starting indexes so that that indexes now create a span\n",
    "    offsets = np.arange(mask_length)[None, None, :]\n",
    "    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n",
    "        batch_size, max_num_masked_span * mask_length\n",
    "    )\n",
    "    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n",
    "\n",
    "    # ensure that we cannot have indices larger than sequence_length\n",
    "    if spec_aug_mask_idxs.max() > sequence_length - 1:\n",
    "        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n",
    "\n",
    "    # scatter indices to mask\n",
    "    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n",
    "\n",
    "    return spec_aug_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_hidden_states(\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        mask_time_indices: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        mask_time_prob: float = 0.05,\n",
    "        mask_time_length: int = 10,\n",
    "        mask_feature_prob: float = 0.0,\n",
    "        mask_feature_length: int = 10,\n",
    "        min_masks: int = 0,\n",
    "        training: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Masks extracted features along time axis and/or along feature axis according to\n",
    "        [SpecAugment](https://arxiv.org/abs/1904.08779).\n",
    "        \"\"\"\n",
    "        _, _, hidden_size = hidden_states.size()\n",
    "\n",
    "        if mask_time_prob > 0.0 or mask_feature_prob > 0.0:\n",
    "            masked_spec_embed = torch.nn.Parameter(torch.FloatTensor(hidden_size).uniform_())\n",
    "\n",
    "        # generate indices & apply SpecAugment along time axis\n",
    "        batch_size, sequence_length, hidden_size = hidden_states.size()\n",
    "\n",
    "        if mask_time_indices is not None:\n",
    "            # apply SpecAugment along time axis with given mask_time_indices\n",
    "            hidden_states[mask_time_indices] = masked_spec_embed.to(hidden_states.dtype)\n",
    "        elif mask_time_prob > 0 and training:\n",
    "            mask_time_indices = _compute_mask_indices(\n",
    "                (batch_size, sequence_length),\n",
    "                mask_prob=mask_time_prob,\n",
    "                mask_length=mask_time_length,\n",
    "                attention_mask=attention_mask,\n",
    "                min_masks=min_masks,\n",
    "            )\n",
    "            mask_time_indices = torch.tensor(mask_time_indices, device=hidden_states.device, dtype=torch.bool)\n",
    "            hidden_states[mask_time_indices] = masked_spec_embed.to(hidden_states.dtype)\n",
    "\n",
    "        if mask_feature_prob > 0 and training:\n",
    "            # generate indices & apply SpecAugment along feature axis\n",
    "            mask_feature_indices = _compute_mask_indices(\n",
    "                (batch_size, hidden_size),\n",
    "                mask_prob=mask_feature_prob,\n",
    "                mask_length=mask_feature_length,\n",
    "                min_masks=min_masks,\n",
    "            )\n",
    "            mask_feature_indices = torch.tensor(mask_feature_indices, device=hidden_states.device, dtype=torch.bool)\n",
    "            mask_feature_indices = mask_feature_indices[:, None].expand(-1, sequence_length, -1)\n",
    "            hidden_states[mask_feature_indices] = 0\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_states = torch.randn(32, 1024, 128)\n",
    "mask_indices = _compute_mask_indices((32, 1024), 0.15, 10)\n",
    "print(mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_input_states = _mask_hidden_states(input_states, mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784, 512])\n"
     ]
    }
   ],
   "source": [
    "print(_mask_hidden_states(latent_input).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list((raw_input == _mask_hidden_states(raw_input, training=False))[0][0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('perceiver-data2vec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "356f0f1e9d918dde982ecf27c70cbfdd6ae28858b3c646ccfe6075c66f643012"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
